# fgan-Causal-Discovery
# Generative Causal Discovery with Unmeasured Confounding via Bayesian f-GAN

[![Paper](https://img.shields.io/badge/Paper-PDF-red)](./paper.pdf) 
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

> **Note:** The full manuscript is currently under review. You can download the preprint [here](./fgancd.pdf).

## ðŸ“– Abstract

Causal discovery from observational data is challenging, especially in the presence of **unmeasured confounding**. Traditional score such as BIC can be problematic for singular models like ADMGs. We use **Bayes Free Energy** instead.

This project proposes a novel **Bayesian framework** that:
1.  **Marginalizes over weights** using a stochastic generator, focusing purely on learning the binary structure ($S_B, S_\Sigma$).
2.  Minimizes the **Bayes Free Energy** via variational **f-divergence** minimization.
3.  Utilizes **Gumbel-Softmax** for differentiable discrete structure search within an adversarial (f-GAN) training loop.

## ðŸš€ Key Method

### 1. From Bayes Free Energy to f-GAN

In regular statistical models, the Bayesian Information Criterion (BIC) approximates the model evidence well. However, causal models with hidden variables are **singular**. So we aim to directly minimize the negative logarithm of the model evidence, that is, the Bayesian free energy:

$$F(X|M) = -\log q(X|M) = -\log \int q(X|w, M)\phi(w|M)dw$$

Our method bridges **Bayesian Model Selection** and **GANs**:

$$
\underbrace{\min_{M} \mathbb{E}[F(\mathbf{X}|M)]}_{\text{Bayes Free Energy}} \iff \underbrace{\min_{M} D_{f}(P_{data} || q(\cdot|M))}_{\text{KL Divergence}} \iff \underbrace{\min_{G} \max_{D} V(G, D)}_{\text{f-GAN Objective}}
$$

$M=(S_B, S_\Sigma)$ is the binary graph structure we want. We aim to minimize the Bayes Free Energy, which is equivalent to minimizing the KL divergence between the true data distribution and the model's marginal distribution. 

KL divergence is a special case of f-divergence (when the generating function $f(u) = u \log u$).

* **Variational Lower Bound**: According to the f-GAN framework, f-divergence has a variational lower bound:

$$D_f(P||Q) \ge \sup_{T} (\mathbb{E}_{x \sim P}[T(x)] - \mathbb{E}_{x \sim Q}[f^*(T(x))])$$

* $P$: The distribution of the real data.

* $Q$: The distribution of the model generated by the generator.

* $T$: The variational function (served by the **discriminator** neural network).

* $f^*$: The conjugate function of $f$.

By leveraging the **f-GAN framework** [Nowozin et al., 2016], we transform this into a min-max optimization problem:

$$
\min_{G} \max_{D} \mathbb{E}_{x \sim P_{data}}[\log D(x)] + \mathbb{E}_{x \sim Q_G}[\log(1 - D(x))]
$$

### 2. Generative Process with Weight Marginalization

Our generator does not learn fixed weights. Instead, it learns the binary structure of the graph. However, since the discrete structure parameters are not differentiable, the generator instead takes two logits matrices, which indicate the**probability of edges**, as parameters. We then use the Gumbel-Softmax trick to get soft proxies of binary structure from the logits. During training, weights are sampled from a prior distribution $\phi(w)$ to simulate the marginal likelihood integral.

The model distribution generation process is as follows:

1. **Structure Sampling**: Use Gumbel-Softmax to sample differentiable proxies of the binary matrices $S_B, S_\Sigma$ from Logits.

2. **Parameter Prior Sampling**: Randomly sample specific weights $B', \Sigma'$ from a uniform distribution (this corresponds to the parameter integration in the model likelihood).

3. **Masking**: Use $S_B, S_\Sigma$ to select edges with weights. $B^* = S_B \circ B', \Sigma^* =S_\Sigma \circ \Sigma'$

4. **Data Generation**: Generate $X_{fake}$ using the linear SEM model $X=E(I-B^*)^{-1}$.

## ðŸ§ª Experiments (Proof of Concept)

We evaluated the method on synthetic datasets with linear SEMs and unmeasured confounding.We compared fGAN-CD against state-of-the-art differentiable method ABIC

### Case study A
| Method | SHD $\downarrow$ | Skeleton F1 $\uparrow$ | Arrowhead F1 $\uparrow$ |
| :--- | :---: | :---: | :---: |
| ABIC (Baseline) | 3.40 | 0.897 | 0.100 |
| **fGAN-CD (Ours)** | **2.17** | **0.909** | **0.667** |

### Independence Test
We designed a high confounding scenario with ground truth: Directed edges $D = [(0,2)]$ and Bidirected edges $B = [(0,1),(1,2),(1,3),(2,3)]$.  

In the ground truth graph, Node $0$ and Node $3$ are d-separated. So there should be no edges in the PAGs of the learned graphs. We use this as a sanity check.
As shown in Figure 1, the baseline ABIC incorrectly inferred a connection between nodes 0 and 3. The fGAN-CD method, however, correctly identifies that no edge exists between nodes 0 and 3 and recovers the true PAG.
![alt text](./caseB.png)
*See the [Full Paper](./fganxd.pdf) for detailed experimental setup and results.*
