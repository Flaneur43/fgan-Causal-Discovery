# fgan-Causal-Discovery
# Generative Causal Discovery with Unmeasured Confounding via Bayesian f-GAN

[![Paper](https://img.shields.io/badge/Paper-PDF-red)](./paper.pdf) 
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

> **Note:** The full manuscript is currently under review. You can download the preprint [here](./fgancd.pdf).

## ðŸ“– Abstract

Causal discovery from observational data is challenging, especially in the presence of **unmeasured confounding**. Traditional score such as BIC can be problematic for singular models like ADMGs. We use **Bayes Free Energy** instead.

This project proposes a novel **Bayesian framework** that:
1.  **Marginalizes over weights** using a stochastic generator, focusing purely on learning the binary structure <img src="https://latex.codecogs.com/svg.latex?(S_B,%20S_\Sigma)" height="20"/>.
2.  Minimizes the **Bayes Free Energy** via variational **f-divergence** minimization.
3.  Utilizes **Gumbel-Softmax** for differentiable discrete structure search within an adversarial (f-GAN) training loop.

## ðŸš€ Key Method

### 1. From Bayes Free Energy to f-GAN

In regular statistical models, the Bayesian Information Criterion (BIC) approximates the model evidence well. However, causal models with hidden variables are **singular**. So we aim to directly minimize the negative logarithm of the model evidence, that is, the Bayesian free energy:

<div align="center">
  <img src="https://latex.codecogs.com/svg.latex?\displaystyle%20F(X|M)%20=%20-\log%20q(X|M)%20=%20-\log%20\int%20q(X|w,%20M)\phi(w|M)dw" />
</div>

Our method bridges **Bayesian Model Selection** and **GANs**:

<div align="center">
  <img src="https://latex.codecogs.com/svg.latex?\displaystyle%20\underbrace{\min_{M}%20\mathbb{E}[F(\mathbf{X}|M)]}_{\text{Bayes%20Free%20Energy}}%20\iff%20\underbrace{\min_{M}%20D_{f}(P_{data}%20||%20q(\cdot|M))}_{\text{KL%20Divergence}}%20\iff%20\underbrace{\min_{G}%20\max_{D}%20V(G,%20D)}_{\text{f-GAN%20Objective}}" />
</div>

<br>

<img src="https://latex.codecogs.com/svg.latex?M=(S_B,%20S_\Sigma)" height="20" /> is the binary graph structure we want. We aim to minimize the Bayes Free Energy, which is equivalent to minimizing the KL divergence between the true data distribution and the model's marginal distribution. 

KL divergence is a special case of f-divergence, when the generating function <img src="https://latex.codecogs.com/svg.latex?f(u)%20=%20u%20\log%20u" height="20" />.

* **Variational Lower Bound**: According to the f-GAN framework, f-divergence has a variational lower bound:

<div align="center">
  <img src="https://latex.codecogs.com/svg.latex?\displaystyle%20D_f(P||Q)%20\ge%20\sup_{T}%20(\mathbb{E}_{x%20\sim%20P}[T(x)]%20-%20\mathbb{E}_{x%20\sim%20Q}[f^*(T(x))])" />
</div>

* <img src="https://latex.codecogs.com/svg.latex?P" height="12" />: The distribution of the real data.

* <img src="https://latex.codecogs.com/svg.latex?Q" height="14" />: The distribution of the model generated by the generator.

* <img src="https://latex.codecogs.com/svg.latex?T" height="12" />: The variational function (served by the **discriminator** neural network).

* <img src="https://latex.codecogs.com/svg.latex?f^*" height="15" />: The conjugate function of <img src="https://latex.codecogs.com/svg.latex?f" height="14" />.

By leveraging the **f-GAN framework** [Nowozin et al., 2016], we transform this into a min-max optimization problem:

<div align="center">
  <img src="https://latex.codecogs.com/svg.latex?\displaystyle%20\min_{G}%20\max_{D}%20\mathbb{E}_{x%20\sim%20P_{data}}[\log%20D(x)]%20+%20\mathbb{E}_{x%20\sim%20Q_G}[\log(1%20-%20D(x))]" />
</div>

### 2. Generative Process with Weight Marginalization

Our generator does not learn fixed weights. Instead, it learns the binary structure of the graph. However, since the discrete structure parameters are not differentiable, the generator instead takes two logits matrices, which indicate the **probability of edges**, as parameters. We then use the Gumbel-Softmax trick to get soft proxies of binary structure from the logits. During training, weights are sampled from a prior distribution <img src="https://latex.codecogs.com/svg.latex?\phi(w)" height="15" /> to simulate the marginal likelihood integral.

The model distribution generation process is as follows:

1. **Structure Sampling**: Use Gumbel-Softmax to sample differentiable proxies of the binary matrices <img src="https://latex.codecogs.com/svg.latex?S_B,%20S_\Sigma" height="20" /> from Logits.

2. **Parameter Prior Sampling**: Randomly sample specific weights <img src="https://latex.codecogs.com/svg.latex?B',%20\Sigma'" height="20" /> from a uniform distribution (this corresponds to the parameter integration in the model likelihood).

3. **Masking**: Use <img src="https://latex.codecogs.com/svg.latex?S_B,%20S_\Sigma" height="20" /> to select edges with weights. <img src="https://latex.codecogs.com/svg.latex?B^*%20=%20S_B%20\circ%20B',%20\Sigma^*%20=S_\Sigma%20\circ%20\Sigma'" height="20" />

4. **Data Generation**: Generate <img src="https://latex.codecogs.com/svg.latex?X_{fake}" height="20" /> using the linear SEM model <img src="https://latex.codecogs.com/svg.latex?X=E(I-B^*)^{-1}" height="20" />.


## ðŸ§ª Experiments (Proof of Concept)

We evaluated the method on synthetic datasets with linear SEMs and unmeasured confounding.We compared fGAN-CD against state-of-the-art differentiable method ABIC

### Case study A
| Method | SHD $\downarrow$ | Skeleton F1 $\uparrow$ | Arrowhead F1 $\uparrow$ |
| :--- | :---: | :---: | :---: |
| ABIC (Baseline) | 3.40 | 0.897 | 0.100 |
| **fGAN-CD (Ours)** | **2.17** | **0.909** | **0.667** |

### Independence Test
We designed a high confounding scenario with ground truth: Directed edges $D = [(0,2)]$ and Bidirected edges $B = [(0,1),(1,2),(1,3),(2,3)]$.  

In the ground truth graph, Node $0$ and Node $3$ are d-separated. So there should be no edges in the PAGs of the learned graphs. We use this as a sanity check.
As shown in Figure 1, the baseline ABIC incorrectly inferred a connection between nodes 0 and 3. The fGAN-CD method, however, correctly identifies that no edge exists between nodes 0 and 3 and recovers the true PAG.
![alt text](./caseB.png)
*See the [Full Paper](./fganxd.pdf) for detailed experimental setup and results.*
